{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "907fbfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: {} /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import progressbar\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if len(device_name) > 0:\n",
    "    print('Found GPU at: {}', format(device_name))\n",
    "else:\n",
    "    device_name = '/device:CPU:0'\n",
    "    print('No GPU, using {}', format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f1736",
   "metadata": {},
   "source": [
    "converting all radiograph images from rgb to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ee144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_names = os.listdir('dataset/Radiographs/')\n",
    "\n",
    "# bar = progressbar.ProgressBar(maxval=len(image_names), \\\n",
    "#     widgets=[progressbar.Bar('#', 'Converting Images to Grayscale: [', ']', '-'), ' ', progressbar.Percentage()])\n",
    "# bar.start()\n",
    "\n",
    "# for idx, img_name in enumerate(image_names):\n",
    "#     img_path = os.path.join('dataset/Radiographs/', img_name)\n",
    "#     image = cv2.imread(img_path)\n",
    "#     grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     cv2.imwrite(img_path, grayscale_image)\n",
    "#     bar.update(idx+1)\n",
    "\n",
    "# bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e1d6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 images belonging to 1 classes.\n",
      "Found 3200 images belonging to 1 classes.\n",
      "Found 800 images belonging to 1 classes.\n",
      "Found 800 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
    "                                                                validation_split = 0.2,\n",
    "                                                               )\n",
    "mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255, validation_split = 0.2)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "target_size = (512, 512)\n",
    "train_image_generator = image_datagen.flow_from_directory(\n",
    "    './dataset/new_direc',\n",
    "    target_size = target_size,\n",
    "    class_mode = None,\n",
    "    classes = ['images'],\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = batch_size,\n",
    "    seed = 42,\n",
    "    subset = 'training',\n",
    "    shuffle=False,\n",
    "    \n",
    ")\n",
    "\n",
    "train_mask_generator = mask_datagen.flow_from_directory(\n",
    "    './dataset/new_direc',\n",
    "    target_size = target_size,\n",
    "    class_mode = None,\n",
    "    classes = ['masks'],\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = batch_size,\n",
    "    seed = 42,\n",
    "    subset = 'training',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "val_image_generator = image_datagen.flow_from_directory(\n",
    "    './dataset/new_direc',\n",
    "    target_size = target_size,\n",
    "    class_mode = None,\n",
    "    classes = ['images'],\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = batch_size,\n",
    "    seed = 42,\n",
    "    subset = 'validation',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "val_mask_generator = mask_datagen.flow_from_directory(\n",
    "    './dataset/new_direc',\n",
    "    target_size = target_size,\n",
    "    class_mode = None,\n",
    "    classes = ['masks'],\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = batch_size,\n",
    "    seed = 42,\n",
    "    subset = 'validation',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "train_generator = zip(train_image_generator, train_mask_generator)\n",
    "val_generator = zip(val_image_generator, val_mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for i,m in train_generator:\n",
    "    img,mask = i,m\n",
    "\n",
    "    if n < 5:\n",
    "        fig, axs = plt.subplots(1 , 2, figsize=(20,4))\n",
    "        axs[0].imshow(img[0])\n",
    "        axs[0].set_title('Input Image')\n",
    "        axs[1].imshow(mask[0],cmap='gray')\n",
    "        axs[1].set_title('Mask Image')\n",
    "        plt.show()\n",
    "        n+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffcfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 400\n",
    "# X_train = []\n",
    "# Y_train = []\n",
    "# image_names = os.listdir('dataset/Radiographs/')\n",
    "\n",
    "# for img_name in image_names:\n",
    "#     img_path = os.path.join('dataset/Radiographs/', img_name)\n",
    "#     mask_path = os.path.join('dataset/Segmentation/teeth_mask', img_name)\n",
    "#     image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     if image is not None and mask is not None:\n",
    "#         resized_image = cv2.resize(image, (769, 400), interpolation=cv2.INTER_LINEAR)\n",
    "#         resized_mask = cv2.resize(mask, (580, 212), interpolation=cv2.INTER_LINEAR)\n",
    "#         (thresh, im_bw) = cv2.threshold(resized_mask, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "#         X_train.append(resized_image/255)\n",
    "#         Y_train.append(im_bw/255)\n",
    "#         count = count - 1\n",
    "#     if count == 0:\n",
    "#         break\n",
    "# X_train = np.float32(np.array(X_train))\n",
    "# Y_train = np.float32(np.array(Y_train))\n",
    "# # Y_train = tf.one_hot(Y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987bb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def iou_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "    union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n",
    "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "    return iou\n",
    "\n",
    "def iou_coef_loss(y_true, y_pred):\n",
    "    return -iou_coef(y_true, y_pred)\n",
    "\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = label(y_true_in > 0.5)\n",
    "    y_pred = label(y_pred_in > 0.5)\n",
    "    \n",
    "    true_objects = len(np.unique(labels))\n",
    "    pred_objects = len(np.unique(y_pred))\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.array(np.mean(metric), dtype=np.float32)\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    metric_value = tf.compat.v1.py_func(iou_metric_batch, [label, pred], tf.float32)\n",
    "    return metric_value\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred,smooth = 100):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device_name):\n",
    "    def buildModel(base_neuron_count, input_shape):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "        c0 = layers.Conv2D(base_neuron_count, activation='relu', kernel_size=3, name=\"c0\")(inputs) # size would be (838, 1613, 64)\n",
    "        c1 = layers.Conv2D(base_neuron_count, activation='relu', kernel_size=3, name=\"c1\")(c0) #(835, 1611, 64)\n",
    "\n",
    "        c2 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid', name=\"c2\")(c1)\n",
    "        c3 = layers.Conv2D(2*base_neuron_count, activation='relu', kernel_size=3, name=\"c3\")(c2)\n",
    "        c4 = layers.Conv2D(2*base_neuron_count, activation='relu', kernel_size=3, name=\"c4\")(c3)\n",
    "\n",
    "        c5 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid', name=\"c5\")(c4)\n",
    "        c6 = layers.Conv2D(4*base_neuron_count, activation='relu', kernel_size=3, name=\"c6\")(c5)\n",
    "        c7 = layers.Conv2D(4*base_neuron_count, activation='relu', kernel_size=3, name=\"c7\")(c6)\n",
    "\n",
    "        c8 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid', name=\"c8\")(c7)\n",
    "        c9 = layers.Conv2D(8*base_neuron_count, activation='relu', kernel_size=3, name=\"c9\")(c8)\n",
    "        c10 = layers.Conv2D(8*base_neuron_count, activation='relu', kernel_size=3, name=\"c10\")(c9)\n",
    "\n",
    "        c11 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid', name=\"c11\")(c10)\n",
    "        c12 = layers.Conv2D(16*base_neuron_count, activation='relu', kernel_size=3, name=\"c12\")(c11)\n",
    "        c13 = layers.Conv2D(16*base_neuron_count, activation='relu', kernel_size=3, name=\"c13\")(c12)\n",
    "\n",
    "        t01 = layers.Conv2DTranspose(512, kernel_size=2, strides=(2, 2), activation='relu', name=\"t01\")(c13)\n",
    "        crop01 = layers.Cropping2D(cropping=((4, 4), (4, 4)), name=\"crop01\")(c10)\n",
    "        concat01 = layers.concatenate([t01, crop01], axis=-1, name=\"concat01\")\n",
    "        c14 = layers.Conv2D(8*base_neuron_count, activation='relu', kernel_size=3, name=\"c14\")(concat01)\n",
    "        c15 = layers.Conv2D(8*base_neuron_count, activation='relu', kernel_size=3, name=\"c15\")(c14)\n",
    "\n",
    "        t02 = layers.Conv2DTranspose(256, kernel_size=2, strides=(2, 2), activation='relu', name=\"t02\")(c15)\n",
    "        crop02 = layers.Cropping2D(cropping=((17, 16), (17, 16)), name=\"crop02\")(c7)\n",
    "        concat02 = layers.concatenate([t02, crop02], axis=-1, name=\"concat02\")\n",
    "        c16 = layers.Conv2D(4*base_neuron_count, activation='relu', kernel_size=3, name=\"c16\")(concat02)\n",
    "        c17 = layers.Conv2D(4*base_neuron_count, activation='relu', kernel_size=3, name=\"c17\")(c16)\n",
    "\n",
    "        t03 = layers.Conv2DTranspose(128, kernel_size=2, strides=(2, 2), activation='relu', name=\"t03\")(c17)\n",
    "        crop03 = layers.Cropping2D(cropping=((41, 41), (41, 41)), name=\"crop03\")(c4)\n",
    "        concat03 = layers.concatenate([t03, crop03], axis=-1, name=\"concat03\")\n",
    "        c18 = layers.Conv2D(2*base_neuron_count, activation='relu', kernel_size=3, name=\"c18\")(concat03)\n",
    "        c19 = layers.Conv2D(2*base_neuron_count, activation='relu', kernel_size=3, name=\"c19\")(c18)\n",
    "\n",
    "        t04 = layers.Conv2DTranspose(64, kernel_size=2, strides=(2, 2), activation='relu', name=\"t04\")(c19)\n",
    "        crop04 = layers.Cropping2D(cropping=((90, 90), (90, 90)), name=\"crop04\")(c1)\n",
    "        concat04 = layers.concatenate([t04, crop04], axis=-1, name=\"concat04\")\n",
    "        c20 = layers.Conv2D(base_neuron_count, activation='relu', kernel_size=3, name=\"c20\")(concat04)\n",
    "        c21 = layers.Conv2D(base_neuron_count, activation='relu', kernel_size=3, name=\"c21\")(c20)\n",
    "\n",
    "        outputs = layers.Conv2D(1, kernel_size=1, padding=\"same\", activation=\"sigmoid\")(c21)\n",
    "        return keras.Model(inputs = inputs, outputs=outputs, name='U-net')\n",
    "    unet = buildModel(2, (512, 512, 1))\n",
    "    unet.summary()\n",
    "    unet.compile()\n",
    "    unet.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "             loss=['binary_crossentropy'], metrics=[iou_coef,'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d84187",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"unet.h5\"\n",
    "checkpoint = ModelCheckpoint(model_path,\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 5,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "\n",
    "\n",
    "training_samples_size = train_image_generator.samples\n",
    "val_samples_size = val_image_generator.samples\n",
    "\n",
    "history = unet.fit(train_generator,\n",
    "                    steps_per_epoch=training_samples_size//batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=val_samples_size//batch_size,\n",
    "                    epochs=10,\n",
    "                    callbacks = [earlystop, checkpoint]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return dice\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred, smooth=1):\n",
    "    return 1 - dice_coef(y_true, y_pred, smooth)\n",
    "\n",
    "def IoULoss(targets, inputs, smooth=1e-6):\n",
    "    \n",
    "    #flatten label and prediction tensors\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    intersection = K.sum(targets*inputs)\n",
    "    total = K.sum(targets) + K.sum(inputs)\n",
    "    union = total - intersection\n",
    "    \n",
    "    IoU = (intersection + smooth) / (union + smooth)\n",
    "    return 1 - IoU\n",
    "\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = label(y_true_in > 0.5)\n",
    "    y_pred = label(y_pred_in > 0.5)\n",
    "    \n",
    "    true_objects = len(np.unique(labels))\n",
    "    pred_objects = len(np.unique(y_pred))\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.array(np.mean(metric), dtype=np.float32)\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    metric_value = tf.compat.v1.py_func(iou_metric_batch, [label, pred], tf.float32)\n",
    "    return metric_value\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Dice similarity coefficient loss, brought to you by: https://github.com/nabsabraham/focal-tversky-unet\n",
    "def dsc(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = Flatten()(y_true)\n",
    "    y_pred_f = Flatten()(y_pred)\n",
    "    intersection = reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dsc(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61cd8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512, 512, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 512, 512, 4)  40          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 512, 512, 4)  16         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 512, 512, 4)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 512, 512, 4)  148         ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512, 512, 4)  16         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 512, 512, 4)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 256, 256, 4)  0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256, 256, 4)  0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 256, 256, 8)  296         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 256, 256, 8)  32         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 256, 256, 8)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 256, 256, 8)  584         ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 256, 256, 8)  32         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 256, 256, 8)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 8)  0          ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128, 128, 8)  0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 128, 128, 16  1168        ['dropout_1[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 128, 128, 16  64         ['conv2d_4[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 128, 128, 16  0           ['batch_normalization_4[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 128, 128, 16  2320        ['activation_4[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 128, 128, 16  64         ['conv2d_5[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 128, 128, 16  0           ['batch_normalization_5[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 16)  0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64, 64, 16)   0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 64, 32)   4640        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64, 64, 32)  128         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 64, 64, 32)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 64, 32)   9248        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64, 64, 32)  128         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 64, 64, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 32)  0           ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 32, 32, 32)   0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 64)   18496       ['dropout_3[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 64)   36928       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 64, 64, 32)  8224        ['activation_9[0][0]']           \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64, 64, 64)   0           ['conv2d_transpose[0][0]',       \n",
      "                                                                  'activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64, 64, 64)   0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 64, 64, 32)   18464       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 64, 64, 32)  128         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 64, 64, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 64, 64, 32)  128         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 128, 128, 16  2064       ['activation_11[0][0]']          \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 128, 128, 32  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                )                                 'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 128, 128, 32  0           ['concatenate_1[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 128, 128, 16  4624        ['dropout_5[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 128, 128, 16  64         ['conv2d_12[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 128, 128, 16  0           ['batch_normalization_12[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 128, 128, 16  2320        ['activation_12[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 128, 128, 16  64         ['conv2d_13[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 128, 128, 16  0           ['batch_normalization_13[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 256, 256, 8)  520        ['activation_13[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 256, 256, 16  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                )                                 'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 256, 256, 16  0           ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 256, 256, 8)  1160        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 256, 256, 8)  32         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 256, 256, 8)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 256, 256, 8)  584         ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 256, 256, 8)  32         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " activation_15 (Activation)     (None, 256, 256, 8)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 512, 512, 4)  132        ['activation_15[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 512, 512, 8)  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                                                  'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 512, 512, 8)  0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 512, 512, 4)  292         ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 512, 512, 4)  16         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 512, 512, 4)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 512, 512, 4)  148         ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 512, 512, 4)  16         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 512, 512, 4)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 512, 512, 1)  5           ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 123,125\n",
      "Trainable params: 122,389\n",
      "Non-trainable params: 736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building Unet by dividing encoder and decoder into blocks\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation, MaxPool2D, Concatenate\n",
    "\n",
    "with tf.device(device_name):\n",
    "    def conv_block(input, num_filters):\n",
    "        x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "        x = BatchNormalization()(x)   #Not in the original network. \n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "        x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)  #Not in the original network\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    #Encoder block: Conv block followed by maxpooling\n",
    "\n",
    "\n",
    "    def encoder_block(input, num_filters):\n",
    "        x = conv_block(input, num_filters)\n",
    "        p = MaxPool2D((2, 2))(x)\n",
    "        p = Dropout(0.25)(p)\n",
    "        return x, p   \n",
    "\n",
    "    #Decoder block\n",
    "    #skip features gets input from encoder for concatenation\n",
    "\n",
    "    def decoder_block(input, skip_features, num_filters):\n",
    "        x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "        x = Concatenate()([x, skip_features])\n",
    "        x = Dropout(0.25)(x)\n",
    "        x = conv_block(x, num_filters)\n",
    "        return x\n",
    "\n",
    "    #Build Unet using the blocks\n",
    "    def build_unet(base_num_features, input_shape, n_classes):\n",
    "        inputs = Input(input_shape)\n",
    "\n",
    "        s1, p1 = encoder_block(inputs, base_num_features)\n",
    "        s2, p2 = encoder_block(p1, 2*base_num_features)\n",
    "        s3, p3 = encoder_block(p2, 4*base_num_features)\n",
    "\n",
    "        s4, p4 = encoder_block(p3, 8*base_num_features)\n",
    "\n",
    "        b1 = conv_block(p4, 16*base_num_features) #Bridge\n",
    "#         b1 = conv_block(p3, 8*base_num_features) #Bridge\n",
    "\n",
    "        d1 = decoder_block(b1, s4, 8*base_num_features)\n",
    "\n",
    "        d2 = decoder_block(d1, s3, 4*base_num_features)\n",
    "#         d2 = decoder_block(b1, s3, 4*base_num_features)\n",
    "        d3 = decoder_block(d2, s2, 2*base_num_features)\n",
    "        d4 = decoder_block(d3, s1, base_num_features)\n",
    "\n",
    "    #     if n_classes == 1:  #Binary\n",
    "    #       activation = 'sigmoid'\n",
    "    #     else:\n",
    "    #       activation = 'softmax'\n",
    "\n",
    "        outputs = Conv2D(n_classes, 1, activation='sigmoid')(d4)  #Change the activation based on n_classes\n",
    "    #     print(activation)\n",
    "\n",
    "        model = Model(inputs, outputs, name=\"U-Net\")\n",
    "        return model\n",
    "\n",
    "    unet = build_unet(4, (512, 512, 1), n_classes=1)\n",
    "    unet.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryIoU(target_class_ids=[1, 0], threshold=0.4)])\n",
    "    unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4fd697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.3968 - binary_io_u: 0.4573\n",
      "Epoch 00001: val_loss improved from inf to 0.30693, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 131s 308ms/step - loss: 0.3968 - binary_io_u: 0.4573 - val_loss: 0.3069 - val_binary_io_u: 0.5683\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.1378 - binary_io_u: 0.7813\n",
      "Epoch 00002: val_loss improved from 0.30693 to 0.13570, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 126s 314ms/step - loss: 0.1378 - binary_io_u: 0.7813 - val_loss: 0.1357 - val_binary_io_u: 0.7901\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0922 - binary_io_u: 0.8066\n",
      "Epoch 00003: val_loss improved from 0.13570 to 0.08616, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 127s 318ms/step - loss: 0.0922 - binary_io_u: 0.8066 - val_loss: 0.0862 - val_binary_io_u: 0.8272\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0783 - binary_io_u: 0.8170\n",
      "Epoch 00004: val_loss improved from 0.08616 to 0.07665, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 130s 324ms/step - loss: 0.0783 - binary_io_u: 0.8170 - val_loss: 0.0767 - val_binary_io_u: 0.8352\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0714 - binary_io_u: 0.8225\n",
      "Epoch 00005: val_loss improved from 0.07665 to 0.06999, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 135s 339ms/step - loss: 0.0714 - binary_io_u: 0.8225 - val_loss: 0.0700 - val_binary_io_u: 0.8355\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0677 - binary_io_u: 0.8254\n",
      "Epoch 00006: val_loss improved from 0.06999 to 0.06417, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 133s 332ms/step - loss: 0.0677 - binary_io_u: 0.8254 - val_loss: 0.0642 - val_binary_io_u: 0.8320\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0650 - binary_io_u: 0.8279\n",
      "Epoch 00007: val_loss did not improve from 0.06417\n",
      "400/400 [==============================] - 136s 340ms/step - loss: 0.0650 - binary_io_u: 0.8279 - val_loss: 0.0658 - val_binary_io_u: 0.8395\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0628 - binary_io_u: 0.8297\n",
      "Epoch 00008: val_loss improved from 0.06417 to 0.06311, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 133s 332ms/step - loss: 0.0628 - binary_io_u: 0.8297 - val_loss: 0.0631 - val_binary_io_u: 0.8396\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0616 - binary_io_u: 0.8310\n",
      "Epoch 00009: val_loss did not improve from 0.06311\n",
      "400/400 [==============================] - 127s 318ms/step - loss: 0.0616 - binary_io_u: 0.8310 - val_loss: 0.0649 - val_binary_io_u: 0.8413\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0600 - binary_io_u: 0.8326\n",
      "Epoch 00010: val_loss improved from 0.06311 to 0.06030, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 127s 318ms/step - loss: 0.0600 - binary_io_u: 0.8326 - val_loss: 0.0603 - val_binary_io_u: 0.8375\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0593 - binary_io_u: 0.8333\n",
      "Epoch 00011: val_loss did not improve from 0.06030\n",
      "400/400 [==============================] - 126s 316ms/step - loss: 0.0593 - binary_io_u: 0.8333 - val_loss: 0.0624 - val_binary_io_u: 0.8405\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0584 - binary_io_u: 0.8344\n",
      "Epoch 00012: val_loss improved from 0.06030 to 0.05909, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 128s 321ms/step - loss: 0.0584 - binary_io_u: 0.8344 - val_loss: 0.0591 - val_binary_io_u: 0.8390\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0582 - binary_io_u: 0.8346\n",
      "Epoch 00013: val_loss improved from 0.05909 to 0.05895, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 133s 333ms/step - loss: 0.0582 - binary_io_u: 0.8346 - val_loss: 0.0589 - val_binary_io_u: 0.8411\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0568 - binary_io_u: 0.8361\n",
      "Epoch 00014: val_loss improved from 0.05895 to 0.05884, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 130s 324ms/step - loss: 0.0568 - binary_io_u: 0.8361 - val_loss: 0.0588 - val_binary_io_u: 0.8402\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0563 - binary_io_u: 0.8367\n",
      "Epoch 00015: val_loss did not improve from 0.05884\n",
      "400/400 [==============================] - 126s 315ms/step - loss: 0.0563 - binary_io_u: 0.8367 - val_loss: 0.0611 - val_binary_io_u: 0.8421\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0560 - binary_io_u: 0.8370\n",
      "Epoch 00016: val_loss improved from 0.05884 to 0.05863, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 127s 318ms/step - loss: 0.0560 - binary_io_u: 0.8370 - val_loss: 0.0586 - val_binary_io_u: 0.8409\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0555 - binary_io_u: 0.8374\n",
      "Epoch 00017: val_loss did not improve from 0.05863\n",
      "400/400 [==============================] - 133s 332ms/step - loss: 0.0555 - binary_io_u: 0.8374 - val_loss: 0.0595 - val_binary_io_u: 0.8411\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0559 - binary_io_u: 0.8373\n",
      "Epoch 00018: val_loss did not improve from 0.05863\n",
      "400/400 [==============================] - 127s 317ms/step - loss: 0.0559 - binary_io_u: 0.8373 - val_loss: 0.0594 - val_binary_io_u: 0.8249\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0551 - binary_io_u: 0.8378\n",
      "Epoch 00019: val_loss improved from 0.05863 to 0.05680, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.0551 - binary_io_u: 0.8378 - val_loss: 0.0568 - val_binary_io_u: 0.8408\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0544 - binary_io_u: 0.8386\n",
      "Epoch 00020: val_loss improved from 0.05680 to 0.05622, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 124s 311ms/step - loss: 0.0544 - binary_io_u: 0.8386 - val_loss: 0.0562 - val_binary_io_u: 0.8420\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0538 - binary_io_u: 0.8390\n",
      "Epoch 00021: val_loss improved from 0.05622 to 0.05553, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 126s 315ms/step - loss: 0.0538 - binary_io_u: 0.8390 - val_loss: 0.0555 - val_binary_io_u: 0.8394\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0535 - binary_io_u: 0.8395\n",
      "Epoch 00022: val_loss did not improve from 0.05553\n",
      "400/400 [==============================] - 125s 313ms/step - loss: 0.0535 - binary_io_u: 0.8395 - val_loss: 0.0559 - val_binary_io_u: 0.8420\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0531 - binary_io_u: 0.8397\n",
      "Epoch 00023: val_loss did not improve from 0.05553\n",
      "400/400 [==============================] - 127s 316ms/step - loss: 0.0531 - binary_io_u: 0.8397 - val_loss: 0.0560 - val_binary_io_u: 0.8395\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0530 - binary_io_u: 0.8399\n",
      "Epoch 00024: val_loss did not improve from 0.05553\n",
      "400/400 [==============================] - 126s 315ms/step - loss: 0.0530 - binary_io_u: 0.8399 - val_loss: 0.0573 - val_binary_io_u: 0.8414\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0529 - binary_io_u: 0.8400\n",
      "Epoch 00025: val_loss improved from 0.05553 to 0.05550, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 126s 315ms/step - loss: 0.0529 - binary_io_u: 0.8400 - val_loss: 0.0555 - val_binary_io_u: 0.8403\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0525 - binary_io_u: 0.8405\n",
      "Epoch 00026: val_loss improved from 0.05550 to 0.05497, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 127s 317ms/step - loss: 0.0525 - binary_io_u: 0.8405 - val_loss: 0.0550 - val_binary_io_u: 0.8417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0524 - binary_io_u: 0.8406\n",
      "Epoch 00027: val_loss did not improve from 0.05497\n",
      "400/400 [==============================] - 127s 316ms/step - loss: 0.0524 - binary_io_u: 0.8406 - val_loss: 0.0574 - val_binary_io_u: 0.8399\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0521 - binary_io_u: 0.8411\n",
      "Epoch 00028: val_loss did not improve from 0.05497\n",
      "400/400 [==============================] - 126s 316ms/step - loss: 0.0521 - binary_io_u: 0.8411 - val_loss: 0.0551 - val_binary_io_u: 0.8442\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0517 - binary_io_u: 0.8416\n",
      "Epoch 00029: val_loss did not improve from 0.05497\n",
      "400/400 [==============================] - 126s 316ms/step - loss: 0.0517 - binary_io_u: 0.8416 - val_loss: 0.0551 - val_binary_io_u: 0.8434\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0514 - binary_io_u: 0.8419\n",
      "Epoch 00030: val_loss improved from 0.05497 to 0.05403, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 127s 318ms/step - loss: 0.0514 - binary_io_u: 0.8419 - val_loss: 0.0540 - val_binary_io_u: 0.8422\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0513 - binary_io_u: 0.8421\n",
      "Epoch 00031: val_loss did not improve from 0.05403\n",
      "400/400 [==============================] - 127s 317ms/step - loss: 0.0513 - binary_io_u: 0.8421 - val_loss: 0.0556 - val_binary_io_u: 0.8431\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0512 - binary_io_u: 0.8423\n",
      "Epoch 00032: val_loss improved from 0.05403 to 0.05378, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 126s 316ms/step - loss: 0.0512 - binary_io_u: 0.8423 - val_loss: 0.0538 - val_binary_io_u: 0.8390\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0511 - binary_io_u: 0.8423\n",
      "Epoch 00033: val_loss improved from 0.05378 to 0.05340, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 131s 327ms/step - loss: 0.0511 - binary_io_u: 0.8423 - val_loss: 0.0534 - val_binary_io_u: 0.8416\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0509 - binary_io_u: 0.8425\n",
      "Epoch 00034: val_loss did not improve from 0.05340\n",
      "400/400 [==============================] - 129s 323ms/step - loss: 0.0509 - binary_io_u: 0.8425 - val_loss: 0.0535 - val_binary_io_u: 0.8422\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0506 - binary_io_u: 0.8428\n",
      "Epoch 00035: val_loss did not improve from 0.05340\n",
      "400/400 [==============================] - 125s 312ms/step - loss: 0.0506 - binary_io_u: 0.8428 - val_loss: 0.0547 - val_binary_io_u: 0.8411\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0505 - binary_io_u: 0.8431\n",
      "Epoch 00036: val_loss did not improve from 0.05340\n",
      "400/400 [==============================] - 125s 313ms/step - loss: 0.0505 - binary_io_u: 0.8431 - val_loss: 0.0549 - val_binary_io_u: 0.8405\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0505 - binary_io_u: 0.8432\n",
      "Epoch 00037: val_loss did not improve from 0.05340\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.0505 - binary_io_u: 0.8432 - val_loss: 0.0540 - val_binary_io_u: 0.8442\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0504 - binary_io_u: 0.8433\n",
      "Epoch 00038: val_loss did not improve from 0.05340\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.0504 - binary_io_u: 0.8433 - val_loss: 0.0544 - val_binary_io_u: 0.8455\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0500 - binary_io_u: 0.8436\n",
      "Epoch 00039: val_loss did not improve from 0.05340\n",
      "400/400 [==============================] - 122s 306ms/step - loss: 0.0500 - binary_io_u: 0.8436 - val_loss: 0.0551 - val_binary_io_u: 0.8440\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0500 - binary_io_u: 0.8438\n",
      "Epoch 00040: val_loss did not improve from 0.05340\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.0500 - binary_io_u: 0.8438 - val_loss: 0.0550 - val_binary_io_u: 0.8457\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0499 - binary_io_u: 0.8440\n",
      "Epoch 00041: val_loss improved from 0.05340 to 0.05326, saving model to image512_bce_iou_4.h5\n",
      "400/400 [==============================] - 125s 311ms/step - loss: 0.0499 - binary_io_u: 0.8440 - val_loss: 0.0533 - val_binary_io_u: 0.8445\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0496 - binary_io_u: 0.8443\n",
      "Epoch 00042: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 125s 312ms/step - loss: 0.0496 - binary_io_u: 0.8443 - val_loss: 0.0541 - val_binary_io_u: 0.8457\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0496 - binary_io_u: 0.8444\n",
      "Epoch 00043: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 124s 311ms/step - loss: 0.0496 - binary_io_u: 0.8444 - val_loss: 0.0541 - val_binary_io_u: 0.8454\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0495 - binary_io_u: 0.8444\n",
      "Epoch 00044: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 125s 314ms/step - loss: 0.0495 - binary_io_u: 0.8444 - val_loss: 0.0558 - val_binary_io_u: 0.8467\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0496 - binary_io_u: 0.8444\n",
      "Epoch 00045: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 124s 311ms/step - loss: 0.0496 - binary_io_u: 0.8444 - val_loss: 0.0543 - val_binary_io_u: 0.8461\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0494 - binary_io_u: 0.8446\n",
      "Epoch 00046: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 125s 311ms/step - loss: 0.0494 - binary_io_u: 0.8446 - val_loss: 0.0544 - val_binary_io_u: 0.8463\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0493 - binary_io_u: 0.8448\n",
      "Epoch 00047: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 124s 310ms/step - loss: 0.0493 - binary_io_u: 0.8448 - val_loss: 0.0536 - val_binary_io_u: 0.8446\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0491 - binary_io_u: 0.8450\n",
      "Epoch 00048: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 125s 312ms/step - loss: 0.0491 - binary_io_u: 0.8450 - val_loss: 0.0539 - val_binary_io_u: 0.8461\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0490 - binary_io_u: 0.8452\n",
      "Epoch 00049: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 126s 314ms/step - loss: 0.0490 - binary_io_u: 0.8452 - val_loss: 0.0538 - val_binary_io_u: 0.8474\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0491 - binary_io_u: 0.8452\n",
      "Epoch 00050: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 126s 316ms/step - loss: 0.0491 - binary_io_u: 0.8452 - val_loss: 0.0538 - val_binary_io_u: 0.8450\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0489 - binary_io_u: 0.8453\n",
      "Epoch 00051: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 126s 315ms/step - loss: 0.0489 - binary_io_u: 0.8453 - val_loss: 0.0562 - val_binary_io_u: 0.8457\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0487 - binary_io_u: 0.8456\n",
      "Epoch 00052: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 125s 312ms/step - loss: 0.0487 - binary_io_u: 0.8456 - val_loss: 0.0558 - val_binary_io_u: 0.8474\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0489 - binary_io_u: 0.8455\n",
      "Epoch 00053: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 126s 314ms/step - loss: 0.0489 - binary_io_u: 0.8455 - val_loss: 0.0552 - val_binary_io_u: 0.8434\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0486 - binary_io_u: 0.8458\n",
      "Epoch 00054: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 124s 310ms/step - loss: 0.0486 - binary_io_u: 0.8458 - val_loss: 0.0538 - val_binary_io_u: 0.8488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0486 - binary_io_u: 0.8459\n",
      "Epoch 00055: val_loss did not improve from 0.05326\n",
      "400/400 [==============================] - 129s 323ms/step - loss: 0.0486 - binary_io_u: 0.8459 - val_loss: 0.0534 - val_binary_io_u: 0.8471\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0485 - binary_io_u: 0.8459\n",
      "Epoch 00056: val_loss did not improve from 0.05326\n",
      "Restoring model weights from the end of the best epoch.\n",
      "400/400 [==============================] - 129s 324ms/step - loss: 0.0485 - binary_io_u: 0.8459 - val_loss: 0.0536 - val_binary_io_u: 0.8483\n",
      "Epoch 00056: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_path = \"image512_bce_iou_4.h5\"\n",
    "checkpoint = ModelCheckpoint(model_path,\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 15,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "\n",
    "\n",
    "training_samples_size = train_image_generator.samples\n",
    "val_samples_size = val_image_generator.samples\n",
    "\n",
    "history = unet.fit(train_generator,\n",
    "                    steps_per_epoch=training_samples_size//batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=val_samples_size//batch_size,\n",
    "                    epochs=100,\n",
    "                    callbacks = [checkpoint, earlystop]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d182dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def iou_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true *y_pred), axis = [1, 2, 3])\n",
    "    union = K.sum(y_true, [1, 2, 3]) + K.sum(y_pred, [1, 2, 3]) - intersection\n",
    "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0313b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for i,m in val_generator:\n",
    "    img,mask = i,m\n",
    "\n",
    "    if n < 1:\n",
    "        fig, axs = plt.subplots(1 , 2, figsize=(20,4))\n",
    "        axs[0].imshow(img[0])\n",
    "        axs[0].set_title('Input Image')\n",
    "        axs[1].imshow(mask[0],cmap='gray')\n",
    "        axs[1].set_title('Mask Image')\n",
    "        plt.show()\n",
    "        n+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb0eff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 12s 125ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted = unet.predict(val_image_generator, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c86f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5\n",
    "resized_image = cv2.resize(val_image_generator[0][x], (807, 420), interpolation=cv2.INTER_LINEAR)\n",
    "cv2.imshow('image', resized_image)\n",
    "\n",
    "resized_mask = cv2.resize(val_mask_generator[0][x], (807, 420), interpolation=cv2.INTER_LINEAR)\n",
    "cv2.imshow('mask', resized_mask)\n",
    "\n",
    "resized_predicted = cv2.resize(predicted[x], (807, 420), interpolation=cv2.INTER_LINEAR)\n",
    "cv2.imshow('predicted', resized_predicted)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403b417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "i  = 3\n",
    "\n",
    "m = tf.keras.metrics.BinaryIoU(target_class_ids = [0, 1], threshold=0.5)\n",
    "m.update_state(val_mask_generator[0][i], predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7e350f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bada6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[predicted >= 0.5] = 1\n",
    "predicted[predicted < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f18f5c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x294dbe8f040>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['binary_io_u'], label='binary_io_u')\n",
    "plt.plot(history.history['val_binary_io_u'],label='val_binary_io_u')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('newhistory', 'wb') as file_pi:\n",
    "#     pickle.dump(history.history, file_pi)\n",
    "    \n",
    "with open('newhistory', \"rb\") as file_pi:\n",
    "    history = pickle.load(file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0260b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_TP = 0\n",
    "total_TN = 0\n",
    "total_FP = 0\n",
    "total_FN = 0\n",
    "\n",
    "def findTP(msk, p_msk):\n",
    "    count = 0\n",
    "    for i in range(512):\n",
    "        for j in range(512):\n",
    "            if msk[i][j] == 1 and p_msk[i][j] == 1:\n",
    "                count = count + 1\n",
    "                \n",
    "    return count\n",
    "\n",
    "def findTN(msk, p_msk):\n",
    "    count = 0\n",
    "    for i in range(512):\n",
    "        for j in range(512):\n",
    "            if msk[i][j] == 0 and p_msk[i][j] == 0:\n",
    "                count = count + 1\n",
    "                \n",
    "    return count\n",
    "\n",
    "def findFP(msk, p_msk):\n",
    "    count = 0\n",
    "    for i in range(512):\n",
    "        for j in range(512):\n",
    "            if msk[i][j] == 0 and p_msk[i][j] == 1:\n",
    "                count = count + 1\n",
    "                \n",
    "    return count\n",
    "\n",
    "def findFN(msk, p_msk):\n",
    "    count = 0\n",
    "    for i in range(512):\n",
    "        for j in range(512):\n",
    "            if msk[i][j] == 1 and p_msk[i][j] == 0:\n",
    "                count = count + 1\n",
    "                \n",
    "    return count\n",
    "\n",
    "for i in range(99):\n",
    "    for j in range(8):\n",
    "        img = val_image_generator[i][j]\n",
    "        mask = val_mask_generator[i][j]\n",
    "        predicted_mask = predicted[8*i + j]\n",
    "        \n",
    "        total_TP = total_TP + findTP(mask, predicted_mask)\n",
    "        total_TN = total_TN + findTN(mask, predicted_mask)\n",
    "        total_FP = total_FP + findFP(mask, predicted_mask)\n",
    "        total_FN = total_FN + findFN(mask, predicted_mask)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff4ce26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 99\n",
    "for j in range(8):\n",
    "    img = val_image_generator[i][j]\n",
    "    mask = val_mask_generator[i][j]\n",
    "    predicted_mask = predicted[8*i + j]\n",
    "\n",
    "    total_TP = total_TP + findTP(mask, predicted_mask)\n",
    "    total_TN = total_TN + findTN(mask, predicted_mask)\n",
    "    total_FP = total_FP + findFP(mask, predicted_mask)\n",
    "    total_FN = total_FN + findFN(mask, predicted_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0aa089b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9560101626923642\n",
      "0.9345377379457054\n",
      "0.9451520107944277\n"
     ]
    }
   ],
   "source": [
    "precision = total_TP/(total_TP + total_FP)\n",
    "recall = total_TP/(total_TP + total_FN)\n",
    "f1_score = 2*recall*precision/(recall + precision)\n",
    "\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "733c1cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19187384\n",
      "180006579\n",
      "882888\n",
      "1344033\n"
     ]
    }
   ],
   "source": [
    "print(total_TP)\n",
    "print(total_TN)\n",
    "print(total_FP)\n",
    "print(total_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "79f6ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total_TP + total_TN + total_FN + total_FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece91fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"black\" if cm[i, j] > thresh else \"white\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72e65f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_TP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10744\\2152928674.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m plot_confusion_matrix(cm           = np.array([[ total_TP/total,  total_FN/total],\n\u001b[0m\u001b[0;32m      2\u001b[0m                                               [  total_FP/total,  total_TN/total],]), \n\u001b[0;32m      3\u001b[0m                       \u001b[0mnormalize\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                       \u001b[0mtarget_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Positive'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Negative'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                       title        = \"Confusion Matrix\")\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_TP' is not defined"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(cm           = np.array([[ total_TP/total,  total_FN/total],\n",
    "                                              [  total_FP/total,  total_TN/total],]), \n",
    "                      normalize    = False,\n",
    "                      target_names = ['Positive', 'Negative'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "31ee1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = unet.fit(X_train, Y_train, batch_size=4, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adacd476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results = unet.evaluate(X_train, Y_test, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92573b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = unet.predict(X_train, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90efa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted[predicted >= 0.2] = 1\n",
    "# predicted[predicted < 0.2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800bd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow('actual', X[10])\n",
    "cv2.imshow('predicted' ,predicted[6])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 25\n",
    "# X_test = []\n",
    "# Y_test = []\n",
    "# image_names = os.listdir('dataset/Radiographs/')\n",
    "\n",
    "# for idx, img_name in enumerate(image_names):\n",
    "#     if idx > 400:\n",
    "#         img_path = os.path.join('dataset/Radiographs/', img_name)\n",
    "#         mask_path = os.path.join('dataset/Segmentation/teeth_mask', img_name)\n",
    "#         image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "#         mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "#         if image is not None and mask is not None:\n",
    "#             resized_image = cv2.resize(image, (769, 400), interpolation=cv2.INTER_LINEAR)\n",
    "#             resized_mask = cv2.resize(mask, (580, 212), interpolation=cv2.INTER_LINEAR)\n",
    "#             (thresh, im_bw) = cv2.threshold(resized_mask, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "#             X_test.append(resized_image/255)\n",
    "#             Y_test.append(im_bw/255)\n",
    "#             count = count - 1\n",
    "#         if count == 0:\n",
    "#             break\n",
    "# X_test = np.float32(np.array(X_test))\n",
    "# Y_test = np.float32(np.array(Y_test))\n",
    "# # Y_train = tf.one_hot(Y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70785636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30698c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4692db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.evaluate(X_test, Y_test, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def EncoderMiniBlock(inputs, n_filters=32, dropout_prob=0.3, max_pooling=True):\n",
    "#     conv = Conv2D(n_filters,\n",
    "#                  3,\n",
    "#                  activation='relu',\n",
    "#                  padding='same',\n",
    "#                  kernel_initializer='HeNormal')(inputs)\n",
    "#     conv = Conv2D(n_filters,\n",
    "#                  3,\n",
    "#                  activation='relu',\n",
    "#                  padding='same',\n",
    "#                  kernel_intializer='HeNormal')(conv)\n",
    "#     conv = BatchNormalization()(conv, training=False)\n",
    "    \n",
    "#     if dropout_prob > 0:\n",
    "#         conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
    "#     if max_pooling:\n",
    "#         next_layer = tf.keras.layers.MaxPooling2d(pool_size = (2,2))(conv)\n",
    "#     else:\n",
    "#         next_layer = conv\n",
    "    \n",
    "#     skip_connection = conv\n",
    "#     return next_layer, skip_connection\n",
    "\n",
    "# def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=32):\n",
    "#     up = Conv2DTranspose(\n",
    "#                  n_filters,\n",
    "#                  (3,3),\n",
    "#                  strides=(2,2),\n",
    "#                  padding='same')(prev_layer_input)\n",
    "#     merge = concatenate([up, skip_layer_input], axis=3)\n",
    "#     conv = Conv2D(n_filters, \n",
    "#                  3,  \n",
    "#                  activation='relu',\n",
    "#                  padding='same',\n",
    "#                  kernel_initializer='HeNormal')(merge)\n",
    "#     conv = Conv2D(n_filters,\n",
    "#                  3, \n",
    "#                  activation='relu',\n",
    "#                  padding='same',\n",
    "#                  kernel_initializer='HeNormal')(conv)\n",
    "#     return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unet_model(inputs):\n",
    "#     filter_count = 32\n",
    "#     dl1, skip1 = EncoderMiniBlock(inputs, filter_count)\n",
    "#     dl2, skip2 = EncoderMiniBlock(dl1, 2*filter_count)\n",
    "#     dl3, skip3 = EncoderMiniBlock(dl2, 4*filter_count)\n",
    "#     dl4, skip4 = EncoderMiniBlock(dl3, 8*filter_count)\n",
    "    \n",
    "#     ul4 = DecoderMiniBlock(dl4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device(device_name):\n",
    "#     model = keras.Model(inputs = input_layer, outputs=output_layer, name='U-net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5668180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(input_layer, start_neurons):\n",
    "#     conv1 = layers.Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "#     conv1 = layers.Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "#     pool1 = layers.MaxPooling2D((2, 2))(conv1)\n",
    "#     pool1 = layers.Dropout(0.25)(pool1)\n",
    "\n",
    "#     conv2 = layers.Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "#     conv2 = layers.Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "#     pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
    "#     pool2 = layers.Dropout(0.5)(pool2)\n",
    "\n",
    "#     conv3 = layers.Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "#     conv3 = layers.Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "#     pool3 = layers.MaxPooling2D((2, 2))(conv3)\n",
    "#     pool3 = layers.Dropout(0.5)(pool3)\n",
    "\n",
    "#     conv4 = layers.Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "#     conv4 = layers.Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "#     pool4 = layers.MaxPooling2D((2, 2))(conv4)\n",
    "#     pool4 = layers.Dropout(0.5)(pool4)\n",
    "\n",
    "#     # Middle\n",
    "#     convm = layers.Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "#     convm = layers.Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n",
    "    \n",
    "#     deconv4 = layers.Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "#     uconv4 = layers.concatenate([deconv4, conv4])\n",
    "#     uconv4 = layers.Dropout(0.5)(uconv4)\n",
    "#     uconv4 = layers.Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "#     uconv4 = layers.Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "\n",
    "#     deconv3 = layers.Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "#     uconv3 = layers.concatenate([deconv3, conv3])\n",
    "#     uconv3 = layers.Dropout(0.5)(uconv3)\n",
    "#     uconv3 = layers.Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "#     uconv3 = layers.Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "\n",
    "#     deconv2 = layers.Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "#     uconv2 = layers.concatenate([deconv2, conv2])\n",
    "#     uconv2 = layers.Dropout(0.5)(uconv2)\n",
    "#     uconv2 = layers.Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "#     uconv2 = layers.Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "\n",
    "#     deconv1 = layers.Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "#     uconv1 = layers.concatenate([deconv1, conv1])\n",
    "#     uconv1 = layers.Dropout(0.5)(uconv1)\n",
    "#     uconv1 = layers.Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "#     uconv1 = layers.Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    \n",
    "#     output_layer = layers.Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    \n",
    "#     return output_layer\n",
    "\n",
    "# input_layer = keras.Input((128, 128, 1))\n",
    "# output_layer = build_model(input_layer, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab948f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "#              loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94484980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.fit(X_train, Y_train, batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f753eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread('dataset/Radiographs/' + image_names[0], cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow('resized', image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed289b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet = buildModel(64, (210, 403, 1))\n",
    "# unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "#              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30df546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow(\"tp\",Y_train[0])\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # declaring the input layer\n",
    "# # In the original paper the network consisted of only one channel.\n",
    "# inputs = layers.Input(shape=(572, 572, 1))\n",
    "# # first part of the U - contracting part\n",
    "# c0 = layers.Conv2D(64, activation='relu', kernel_size=3)(inputs)\n",
    "# c1 = layers.Conv2D(64, activation='relu', kernel_size=3)(c0)  # This layer for concatenating in the expansive part\n",
    "# c2 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(c1)\n",
    "\n",
    "# c3 = layers.Conv2D(128, activation='relu', kernel_size=3)(c2)\n",
    "# c4 = layers.Conv2D(128, activation='relu', kernel_size=3)(c3)  # This layer for concatenating in the expansive part\n",
    "# c5 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(c4)\n",
    "\n",
    "# c6 = layers.Conv2D(256, activation='relu', kernel_size=3)(c5)\n",
    "# c7 = layers.Conv2D(256, activation='relu', kernel_size=3)(c6)  # This layer for concatenating in the expansive part\n",
    "# c8 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(c7)\n",
    "\n",
    "# c9 = layers.Conv2D(512, activation='relu', kernel_size=3)(c8)\n",
    "# c10 = layers.Conv2D(512, activation='relu', kernel_size=3)(c9)  # This layer for concatenating in the expansive part\n",
    "# c11 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(c10)\n",
    "\n",
    "# c12 = layers.Conv2D(1024, activation='relu', kernel_size=3)(c11)\n",
    "# c13 = layers.Conv2D(1024, activation='relu', kernel_size=3, padding='valid')(c12)\n",
    "\n",
    "# # We will now start the second part of the U - expansive part\n",
    "# t01 = layers.Conv2DTranspose(512, kernel_size=2, strides=(2, 2), activation='relu')(c13)\n",
    "# crop01 = layers.Cropping2D(cropping=(4, 4))(c10)\n",
    "\n",
    "# concat01 = layers.concatenate([t01, crop01], axis=-1)\n",
    "\n",
    "# c14 = layers.Conv2D(512, activation='relu', kernel_size=3)(concat01)\n",
    "# c15 = layers.Conv2D(512, activation='relu', kernel_size=3)(c14)\n",
    "\n",
    "# t02 = layers.Conv2DTranspose(256, kernel_size=2, strides=(2, 2), activation='relu')(c15)\n",
    "# crop02 = layers.Cropping2D(cropping=(16, 16))(c7)\n",
    "\n",
    "# concat02 = layers.concatenate([t02, crop02], axis=-1)\n",
    "\n",
    "# c16 = layers.Conv2D(256, activation='relu', kernel_size=3)(concat02)\n",
    "# c17 = layers.Conv2D(256, activation='relu', kernel_size=3)(c16)\n",
    "\n",
    "# t03 = layers.Conv2DTranspose(128, kernel_size=2, strides=(2, 2), activation='relu')(c17)\n",
    "# crop03 = layers.Cropping2D(cropping=(40, 40))(c4)\n",
    "\n",
    "# concat03 = layers.concatenate([t03, crop03], axis=-1)\n",
    "\n",
    "# c18 = layers.Conv2D(128, activation='relu', kernel_size=3)(concat03)\n",
    "# c19 = layers.Conv2D(128, activation='relu', kernel_size=3)(c18)\n",
    "\n",
    "# t04 = layers.Conv2DTranspose(64, kernel_size=2, strides=(2, 2), activation='relu')(c19)\n",
    "# crop04 = layers.Cropping2D(cropping=(88, 88))(c1)\n",
    "\n",
    "# concat04 = layers.concatenate([t04, crop04], axis=-1)\n",
    "\n",
    "# c20 = layers.Conv2D(64, activation='relu', kernel_size=3)(concat04)\n",
    "# c21 = layers.Conv2D(64, activation='relu', kernel_size=3)(c20)\n",
    "\n",
    "# outputs = layers.Conv2D(2, kernel_size=1)(c21)\n",
    "\n",
    "# model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"u-netmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f011cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143add04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6091b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_classes = 2\n",
    "\n",
    "# labels = tf.ones((443, 300), dtype=tf.int32)\n",
    "# ohe_labels = tf.one_hot(Y_train, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleanName(i):\n",
    "#     return i.lower()\n",
    "\n",
    "\n",
    "# direc = './dataset/Radiographs/'\n",
    "# imgs = os.listdir('./dataset/Radiographs')\n",
    "\n",
    "# for i in imgs:\n",
    "#     os.rename(direc + i, direc + cleanName(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90602ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('./dataset/Radiographs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278e544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
